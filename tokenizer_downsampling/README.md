# Text Classification Pipeline: Custom Tokenizer + Downsampling + Naive Bayes

Проект направлен на исследование влияния размера словаря токенизатора на точность классификации текста. Дополнительно реализована балансировка классов и параллельный препроцессинг текстов с использованием библиотеки `pandarell`.

## Цель

Построить классификационный пайплайн с наивным байесовским классификатором, обучая кастомный токенизатор (архитектура Карпатного) на разных размерах словаря. Провести анализ зависимости точности от размера словаря.

## Этапы работы

- Загрузка `dataset_hw1.csv`
- Балансировка классов с помощью downsampling (по минимальному классу)
- Параллельный препроцессинг с использованием `pandarell`
- Обучение кастомного токенизатора на различных размерах словаря
- Оценка качества модели (наивный байес) для каждого размера словаря
- Построение кривой зависимости точности от количества токенов
- Максимальный размер словаря определён через `TfidfVectorizer` из NLTK

## Используемые технологии

- Python 3.10+
- Pandas
- Pandarell
- scikit-learn
- matplotlib / seaborn
- NLTK
- Кастомный токенизатор (архитектура Андрея Карпатного)

