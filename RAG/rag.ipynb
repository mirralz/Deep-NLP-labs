{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dec43c3-2c78-4a9a-9e4e-2e2b3df710f4",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Имя, Фамилия:\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc645416-9267-4ae6-a3bb-2c6705c6791f",
   "metadata": {},
   "source": [
    "## Часть 1. 6 баллов\n",
    "\n",
    "Нужно взять текущий датасет или любой другой, понравившийся вам в формате question/answer. Не рекомендуется брать датасет больше текущего.\n",
    "Далее необходимо реализовать все функции ниже и сделать всё то, что было на лекции-семинаре. Построить RAG на 2 разных вариантах индекса,\n",
    "замерить качество, сделать вывод о том, какой из вариантов лучше. Качество финальной реализации RAG не обязательно оценивать на большом объеме, достаточно несколько десятков примеров (например до 100), если каждая генерация LLM будет либо слишком долгой или вы ограничены небольшим кол-вом бесплатных токенов (в случае gigachat api).\n",
    "\n",
    "В качестве LLM можно взять open-source сервис или воспользоваться бесплатной квотой в 900_000 [gigachat](https://developers.sber.ru/studio). Функция похода в api гигачат есть в gigachat.py,\n",
    "где нужно лишь добавить свой токен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f440252-4f52-4eb3-9d63-5b9db04df8e9",
   "metadata": {},
   "source": [
    "#### Поднять elasticsearch локально\n",
    "```\n",
    "docker run -p 9200:9200 --rm --env-file elastic.env docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "```\n",
    "содержимое elastic.env\n",
    "```\n",
    "discovery.type=single-node\n",
    "xpack.security.enabled=false\n",
    "```\n",
    "#### Поднять LLM локально\n",
    "```\n",
    "docker run --rm -p 11434:11434 -v ollama:/root/.ollama --name ollama ollama/ollama:0.5.12\n",
    "\n",
    "# в отдельном экране скачиваем нужную модель\n",
    "docker exec -it ollama bash\n",
    "ollama pull <выбранная модель> # выбрать модель https://github.com/ollama/ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd482bf7-d58a-49ad-8e24-61a3d17e9bef",
   "metadata": {},
   "source": [
    "## Часть 2. 4 балла\n",
    "\n",
    "На занятии мы уже обсуждали, что один из возможных способов улучшить knn поиск - это улучшить векторный енкодер. \n",
    "В задании предлагается ровно это и сделать, взять нужный датасет и дообучить его, не забыв про полный цикл задачи\n",
    "машинного обучения (реализовать все необходиомое для обучения, поделить выборки на train/val/test, замерить качество и др.).\n",
    "По итогу качество приближенного поиска должно стать лучше предобученной модели. Базовую модель, технологию и способ дообучения вы выбирайте сами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00b412-31cb-4d38-8135-3111242fa39e",
   "metadata": {},
   "source": [
    "## (Опционально) Часть 3. дополнительные 5 баллов\n",
    "\n",
    "Сверхнормы предлагается 2 опциональных задания:\n",
    "- (4 балла) сделать гибридный поиск (полнотекстовый поиск + приближенный), многие современные поисковые системы (например [baidu](https://arxiv.org/abs/2106.03373)) используют именно такой подход. В идеале это должно работать за 1 поход\n",
    "в поисковый сервис\n",
    "- (1 балл) сделать UI интерфейс к вашей RAG-системе. Подсказка: посмотрите в сторону streamlit. \n",
    "Основная функциональность: ввод текста в специальной строке ввода и ответ RAG-сервиса на одном экране"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb8674-8e34-4ff6-a716-574ad7b99e09",
   "metadata": {},
   "source": [
    "## Домашнее задание № 11\n",
    "\n",
    "Выполните задания, описанные выше:\n",
    "\n",
    "+ Мягкий дедлайн: `19.04.25 23:59`\n",
    "+ Жесткий дедлайн: `26.04.25 23:59` (половина баллов)\n",
    "\n",
    "\n",
    "После жесткого дедлайна задание не принимается."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f4c23-14ee-4a30-bee2-2619a04374b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb555533-379d-4e9d-ae40-a92fc4dea45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from gigachat import get_gigachat_response\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6954ede-a26a-4b7f-b272-80e700ed3e39",
   "metadata": {},
   "source": [
    "# RAG intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809eedc0-c1c8-4a4f-9859-b13d8394ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3.2:1b - 1В\n",
    "# deepseek-r1 - 7В\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8445f-57a4-48b5-aee2-9db3345ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_any_llm(prompt, client, model_name):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def build_english_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You will get corpus of texts and one question. Your aim is finding an exact answer using the given context. Try to give short answer to the question\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, text in enumerate(search_results):\n",
    "        context = context + f\"text_number: {i} text: {text}\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4068cb-36ce-4550-91f3-31276edb28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response_any_llm(\"что такое кошка?\", client, \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d6202-b687-41ab-85a8-1135f8c1e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response_any_llm(\"расскажи о китах\", client, \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688e8d8-9cf5-4534-907d-fc445eefebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response_any_llm(\"what is a cat?\", client, \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca92627-799c-421d-a8bc-5c8dce3dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who won the match sri lanka or india\"\n",
    "context = \"India won the Test series 1–0, after the first and third matches were drawn.[15] India won the ODI series 2–1, their eighth consecutive series win since beating Zimbabwe in June 2016.[16] India won the T20I series 3–0.[17]\"\n",
    "prompt = build_english_prompt(question, [context])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ad397-52a6-43f1-8392-85c6c415a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response_any_llm(prompt, client, \"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed3e18-70cd-46db-a168-db139b5bbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gigachat_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954ce8a-8df5-46bb-aea0-481b98fb3308",
   "metadata": {},
   "source": [
    "# Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56960322-2007-48cd-b19c-bd09d807a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77963603-2131-4d3c-ba5f-9cddbd30256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"kuznetsoffandrey/sberquad\")\n",
    "validation = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3788e-2edd-4064-9f47-6f53f56fc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"context\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "simple_index_name = \"sberquad-index\"\n",
    "\n",
    "es_client.indices.delete(index=simple_index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=simple_index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e28921-77b2-4a05-88a6-b80fdba8ebe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in tqdm.tqdm(validation):\n",
    "    document = {\"context\": doc['context']}\n",
    "    try:\n",
    "        es_client.index(index=simple_index_name, document=document, id=doc['id'])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c3427-bc25-48e0-8f1c-78c8488ddaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализовать функцию запроса в elasticsearch для получения ответа с помощью полнотекстового поиска\n",
    "def search(question, index_name, k):\n",
    "    \"\"\"\n",
    "    question - вопрос\n",
    "    index_name - индекс, в который осуществляется поход\n",
    "    k - кол-во контекстов, которое должно вернуться\n",
    "\n",
    "    Output: набор контекстов, ответ поиска\n",
    "\n",
    "    Подсказка:\n",
    "    query = {\n",
    "        \"match\": {\n",
    "            \"context\": question\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, query=query)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d34fe-4c0c-4cb7-83e5-8f8ad3b967c7",
   "metadata": {},
   "source": [
    "# simple rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae7f78-890f-4eaa-ad74-7b24a6d0d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_russian_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "Тебе будет дан набор текстов и один вопрос. Твоя задача найти точный ответ на на вопрос, имея заданный контекст. Старайся давать короткий ответ на вопрос\n",
    "\n",
    "ВОПРОС: {question}\n",
    "\n",
    "КОНТЕКСТ: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, text in enumerate(search_results):\n",
    "        context = context + f\"text_number: {i} text: {text}\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433257e5-95b2-4cf4-9217-c0dbc125ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализовать функцию простого rag, использующий полнотекстовый поиск\n",
    "def simple_rag(query, top_n_contexts):\n",
    "    pass\n",
    "\n",
    "obj = validation[4500]\n",
    "simple_rag(obj['question'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263469e5-4f40-45b3-9d29-a8aeb598ad3c",
   "metadata": {},
   "source": [
    "# simple evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110f4f31-95ab-4b5f-8bd6-de1d801e18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализовать функцию оценки качества retrieve стадии\n",
    "def precision_at_k(dataset, k, search_func):\n",
    "    \"\"\"\n",
    "    Функция для замера качества поиска с помощью elasticsearch\n",
    "    \n",
    "    dataset - датасет для замера\n",
    "    k - топ на котором замеряется качество\n",
    "    search_func - функция поиска в elasticsearch\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d7c86-0594-444d-ae15-ef84007c5965",
   "metadata": {},
   "source": [
    "# Improved knn-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47125ba-9919-470c-9547-b2a5f5ac7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embedding = model.encode(validation[0][\"question\"])\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1260878-1747-46bc-bcd6-61fc9ea52e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"context\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"integer\"},\n",
    "            \"context_vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"index\": True, \"similarity\": \"cosine\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "knn_index_name = \"sberquad-index-knn\"\n",
    "\n",
    "es_client.indices.delete(index=knn_index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=knn_index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa9946-3b83-42e2-ae17-5b898a5f37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in tqdm.tqdm(enumerate(validation), total=len(validation)):\n",
    "    doc_embed = model.encode(doc[\"context\"]).tolist()\n",
    "    doc = {\"context\": doc['context'], \"id\": doc['id'], \"context_vector\": doc_embed}\n",
    "    try:\n",
    "        es_client.index(index=knn_index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264fee4-8736-417b-90f2-2a6b0315e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализовать функцию запроса в elasticsearch для получения ответа с помощью приближенного поиска\n",
    "def knn_search(question, encoder, index_name, k):\n",
    "    \"\"\"\n",
    "    question - вопрос\n",
    "    encoder - енкодер для векторизации запроса\n",
    "    index_name - индекс, в который осуществляется поход\n",
    "    k - кол-во контекстов, которое должно вернуться\n",
    "\n",
    "    Output: набор контекстов, ответ поиска\n",
    "\n",
    "    Подсказка:\n",
    "    query = {\n",
    "        \"field\": \"context_vector\",\n",
    "        \"query_vector\": embedding,\n",
    "        \"k\": k,\n",
    "        \"num_candidates\": 10\n",
    "    }\n",
    "    response = es_client.knn_search(index=knn_index_name, knn=query)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# реализовать функцию rag, использующий knn поиск\n",
    "def knn_rag(question, top_n_contexts):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633917c8-aeee-47c8-b801-8cc70b5ec0e4",
   "metadata": {},
   "source": [
    "# Evaluate end-2-end RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d058c1-4962-4843-91ca-c2171c51d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_quality(dataset, encoder, search_func):\n",
    "    \"\"\"\n",
    "    dataset - датасет\n",
    "    encoder - енкодер для векторизации запроса\n",
    "    search_func - функция поиска в elasticsearch\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
