{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBA5IPyb4BFH"
      },
      "source": [
        "# LLM\n",
        "\n",
        "Имя, Фамилия:\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnY6pwvpghE"
      },
      "source": [
        "В этой домашней работе вам предоставится уникальная возможность обучить Byte-level BPE токенизатор и небольшую LM.  \n",
        "\n",
        "Домашняя работа состоит из нескольких последовательных блоков: реализация и обучение токенизатора, реализация Transformer модели и обучение модели на датасете с русскими анекдотами!\n",
        "\n",
        "Обученные токенизатор и модель можно и нужно выложить на [🤗 HuggingFace](https://huggingface.co/). Зарегистрируйтесь там, подпишитесь на [deep vk](https://huggingface.co/deepvk) и создайте себе API токен.\n",
        "\n",
        "Следуйте ячейкам тетрадки и заполняйте пропущенные ячейки. В конце тетрадки вы найдете задачи со звездочкой, чтобы получить максимальный балл!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0byNYx5dzB4b"
      },
      "outputs": [],
      "source": [
        "# Установим необходимые дополнительные библиотеки\n",
        "\n",
        "%pip install --quiet datasets livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILR1tu3z9oI"
      },
      "outputs": [],
      "source": [
        "# Необходимые импорты\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache, partial\n",
        "from pathlib import Path\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download\n",
        "from huggingface_hub.utils import SoftTemporaryDirectory\n",
        "from livelossplot import PlotLosses\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7OLSivnPW0"
      },
      "outputs": [],
      "source": [
        "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
        "# Это могут быть как целые функции, так и отдельные части внутри них\n",
        "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
        "\n",
        "\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkJJp2JK99x"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVWKkwaryDTq"
      },
      "outputs": [],
      "source": [
        "# Подготовим репозиторий для будущей модели и токенизатора\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw1\"  # Или как вам хочется\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")\n",
        "\n",
        "# И другие полезные вещи\n",
        "SEED = 0xC0FFEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxN5JUbZ3ToV"
      },
      "source": [
        "# Датасет\n",
        "\n",
        "Первым делом загрузим данные: [🤗 IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)\n",
        "\n",
        "И немного посмотрим на них 👀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT78JNcqpRXW"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"IgorVolochay/russian_jokes\")\n",
        "print(\"\\n===\\n\".join(dataset[\"train\"][\"text\"][:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7G7o6OdK99z"
      },
      "outputs": [],
      "source": [
        "# Подготовим холдауты\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzvdEVO3-kM"
      },
      "source": [
        "# Токенизатор [3 балла]\n",
        "\n",
        "В качестве токенизатора будем использоват Byte-level BPE.\n",
        "\n",
        "Для этого:\n",
        "1. Реализуем его обучения, нам необходимо построить словарь заданного размера и набор слияний по этому словарю\n",
        "2. Обучим токенизатор на датасете\n",
        "3. Реализуем инференс токенизатора: кодирование текста и декодирование токенов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U6H1iLU3kI"
      },
      "outputs": [],
      "source": [
        "# Всякие полезности\n",
        "\n",
        "WHITESPACE_SPLITTER = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "def bytes_to_unicode() -> dict[int, str]:\n",
        "    \"\"\"The original dictionary consists of 256 bytes and their corresponding Unicode characters.\n",
        "    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,\n",
        "    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.\n",
        "    \"\"\"\n",
        "    initial_bytes = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    )\n",
        "    initial_chars = [chr(it) for it in initial_bytes]\n",
        "    n = 0\n",
        "    for byte in range(2**8):\n",
        "        if byte not in initial_bytes:\n",
        "            initial_bytes.append(byte)\n",
        "            initial_chars.append(chr(2**8 + n))\n",
        "            n += 1\n",
        "    return dict(sorted(zip(initial_bytes, initial_chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk919hENEFwL"
      },
      "outputs": [],
      "source": [
        "def merge(merge_pair: tuple[str, str], pair_frequences: Counter[tuple[str, str]], words_by_tokens: Counter[tuple[str]]):\n",
        "    \"\"\"Merges a given pair of tokens and update corresponding stats\n",
        "\n",
        "    Args:\n",
        "        merge_pair: The pair of tokens to be merged.\n",
        "        pair_frequences: A counter tracking the frequency of token pairs in the dataset.\n",
        "        words_by_tokens: A counter mapping tokenized words to their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        Updated pair frequences and word tokenization w.r.t. to new token.\n",
        "    \"\"\"\n",
        "    todo()\n",
        "\n",
        "\n",
        "def train(data: list[str], vocab_size: int = 1024, special_tokens: list[str] = None):\n",
        "    \"\"\"Train BPE tokenizer on passed data\n",
        "\n",
        "    Args:\n",
        "        data: List of train documents\n",
        "        vocab_size: Size of target vocabulary\n",
        "        special_tokens: List of special tokens to add into vocabulary\n",
        "    Returns:\n",
        "        vocabulary: mapping from string token to id\n",
        "        merges: list of merges, each one is tuple of string tokens\n",
        "    \"\"\"\n",
        "    if vocab_size < 256:\n",
        "        raise ValueError(\"Vocab size can't be less than 256\")\n",
        "    if special_tokens is None:\n",
        "        special_tokens = []\n",
        "\n",
        "    # 1. Initialize vocabulary (using inverse one during training)\n",
        "    id2token = bytes_to_unicode()\n",
        "    merges = []\n",
        "\n",
        "    # 2. Load data\n",
        "    words_by_tokens = Counter()\n",
        "    for sample in tqdm(data, desc=\"Loading data\"):\n",
        "        # 2.1 Split into words\n",
        "        words = WHITESPACE_SPLITTER.findall(sample.strip())\n",
        "        for word in words:\n",
        "            # 2.2 Tokenize with base vocabulary\n",
        "            todo()\n",
        "\n",
        "    # 3. Calculate statistic of token's pairs\n",
        "    pair_frequences = Counter()\n",
        "    todo()\n",
        "\n",
        "    # 4. Build vocabulary\n",
        "    pbar = trange(vocab_size, desc=\"Building vocabulary\", initial=len(id2token) + len(special_tokens))\n",
        "    while len(id2token) < vocab_size - len(special_tokens):\n",
        "        if len(pair_frequences) == 0:\n",
        "            print(\"Not enough data to fulfil vocabulary\")\n",
        "            break\n",
        "\n",
        "        # 4.1 Find the most frequent pair and create new token\n",
        "        top_pair = todo()\n",
        "        new_token = todo()\n",
        "        del pair_frequences[top_pair]\n",
        "\n",
        "        # 4.2 Add to vocabulary\n",
        "        if new_token in id2token.values():\n",
        "            continue\n",
        "        id2token[len(id2token)] = new_token\n",
        "        merges.append(top_pair)\n",
        "\n",
        "        # 4.3 Update stats and merge the top pair in all tokens\n",
        "        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)\n",
        "\n",
        "        pbar.update()\n",
        "    pbar.close()\n",
        "\n",
        "    # 5. Add special tokens\n",
        "    for special_token in special_tokens:\n",
        "        id2token[len(id2token)] = special_token\n",
        "\n",
        "    return {v: k for k, v in id2token.items()}, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLwur-KgK990"
      },
      "outputs": [],
      "source": [
        "# Обучаем токенизатор на тренировочных текстах\n",
        "# Для нашей задачи хватит и небольшого словаря, но можете пробовать и большего размера обучить!\n",
        "\n",
        "\n",
        "vocab, merges = train(dataset[\"train\"][\"text\"], vocab_size=1024, special_tokens=[\"[EOS]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsapJUAK990"
      },
      "outputs": [],
      "source": [
        "# Посмотрим на случайные токены\n",
        "\n",
        "random_tokens = [512, 614, 768, 888, 1022]\n",
        "unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}\n",
        "for token_id in random_tokens:\n",
        "    token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "    raw_bytes = bytes([unicode_to_bytes[it] for it in token])\n",
        "    print(f\"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUz7cma1Czs"
      },
      "outputs": [],
      "source": [
        "class ByteLevelBPETokenizer:\n",
        "\n",
        "    def __init__(self, vocab: dict[str, int], merges: list[tuple[str, str]], eos_token: str = \"[EOS]\"):\n",
        "        \"\"\"Byte-Level BPE Tokenizer\n",
        "\n",
        "        Args:\n",
        "            vocab: mapping from string token to id\n",
        "            merges: list of merges in prioritized order\n",
        "            eos_token: string representation of EOS token\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if eos_token not in vocab:\n",
        "            raise ValueError(\"There is no EOS token in vocab\")\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.token2id = vocab\n",
        "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
        "        self.eos_token = eos_token\n",
        "        self.eos_token_id = self.token2id[eos_token]\n",
        "\n",
        "        # The closer the pair is to the beginning, the higher the rank\n",
        "        self.merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "\n",
        "    @lru_cache\n",
        "    def bpe(self, word: tuple[str]) -> tuple[str]:\n",
        "        \"\"\"Process word into tokenized representation.\n",
        "        Word is a tuple of base tokens, i.e. bytes.\n",
        "\n",
        "        Under the hood:\n",
        "        1. Tracks the set of token pairs, bi-grams\n",
        "        2. While possible, replaces the highest-ranking pair with its union\n",
        "\n",
        "        Args:\n",
        "            word: list of base string tokens\n",
        "        Return:\n",
        "            list of BPE tokens\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def encode(self, text: str, add_eos_token: bool = True) -> list[int]:\n",
        "        \"\"\"Convert string to list of token ids.\n",
        "\n",
        "        Args:\n",
        "            text: input string, may contain multiple words\n",
        "            add_eos_token: whether to add eos token id at the end\n",
        "        Return:\n",
        "            list of ints, ids of tokenized text\n",
        "        \"\"\"\n",
        "        words = WHITESPACE_SPLITTER.findall(text)\n",
        "        todo()\n",
        "\n",
        "    def decode(self, idx: list[int]) -> str:\n",
        "        \"\"\"Convert list of tokens' ids to text, opposite to encode method\n",
        "\n",
        "        Args:\n",
        "            idx: list of tokens' ids\n",
        "        Return:\n",
        "            string, decoded text\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def push_to_hub(self, repo_id, *, private=None, token=None):\n",
        "        api = HfApi()\n",
        "        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id\n",
        "\n",
        "        # Push the files to the repo in a single commit\n",
        "        with SoftTemporaryDirectory() as tmp:\n",
        "            save_directory = Path(tmp) / repo_id\n",
        "            save_directory.mkdir(parents=True)\n",
        "            with open(save_directory / \"vocabulary.json\", \"w\") as f_out:\n",
        "                print(json.dumps(self.token2id, indent=2), file=f_out)\n",
        "            with open(save_directory / \"merges.json\", \"w\") as f_out:\n",
        "                print(json.dumps({\"merges\": self.merges}), file=f_out)\n",
        "\n",
        "            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):\n",
        "        if not os.path.isdir(pretrained_model_name_or_path):\n",
        "            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)\n",
        "        else:\n",
        "            storage_folder = pretrained_model_name_or_path\n",
        "        storage_folder = Path(storage_folder)\n",
        "        with open(storage_folder / \"vocabulary.json\", \"r\") as f_in:\n",
        "            vocab = json.load(f_in)\n",
        "        with open(storage_folder / \"merges.json\", \"r\") as f_in:\n",
        "            merges = [tuple(it) for it in json.load(f_in)[\"merges\"]]\n",
        "        return cls(vocab, merges, **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTQzO3wK991"
      },
      "outputs": [],
      "source": [
        "# Инициализируем токенизатор\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(vocab, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9m_65vBK991"
      },
      "outputs": [],
      "source": [
        "# Загружаем токенизатор на хаб\n",
        "\n",
        "tokenizer.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9VohtKrK991"
      },
      "outputs": [],
      "source": [
        "# Скачиваем токенизатор с хаба\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvf_Q8knPW4"
      },
      "outputs": [],
      "source": [
        "# Смотрим на работу токенизатора\n",
        "\n",
        "text = \"Студенты ВШЭ знают, что экономика — это когда на лекции ты теряешь время, а на экзамене — надежду.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "reverse_text = [tokenizer.decode([it]) for it in ids]\n",
        "print(\"|\".join(reverse_text))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgpwFYiK991"
      },
      "outputs": [],
      "source": [
        "# Посчитаем немного статистики по токенизации, определимся с размером контекста у модели\n",
        "\n",
        "lens = []\n",
        "for text in tqdm(dataset[\"test\"][\"text\"]):\n",
        "    ids = tokenizer.encode(text)\n",
        "    lens.append(len(ids))\n",
        "\n",
        "print(f\"Average token len per sample: {sum(lens) / len(lens):.2f}\")\n",
        "print(f\"Minimum and maximum lens are: {min(lens)} and {max(lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwN3mfmznPW5"
      },
      "source": [
        "Должно получиться в среднем по 70 токенов на последовательность.\n",
        "Контекста в 128 токенов будет вполне достаточно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1FljGXC7I-"
      },
      "source": [
        "# Модель [5 баллов]\n",
        "\n",
        "В качестве модели реализуем трансформер, в котором\n",
        "1. В качестве позиционных эмбеддингов используется ALiBi\n",
        "2. Механизм внимания использует GQA\n",
        "3. В Feed-Forward блоке SwiGLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZrCvkmK992"
      },
      "outputs": [],
      "source": [
        "# Для удобства заведем конфиг для модели\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_kv_head: int\n",
        "    hidden_dim: int\n",
        "    intermediate_dim: int\n",
        "    dropout: float = 0.1\n",
        "    vocab_size: int = 1024\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"nano\": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),\n",
        "    \"mini\": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),\n",
        "    \"small\": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYXMb5PEDFkt"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        \"\"\"Root Mean Square Layer Normalization\n",
        "\n",
        "        Args:\n",
        "            dim: Feature dimension\n",
        "            eps: Small constant for numerical stability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        todo()\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Causal Self-Attention with support of\n",
        "        Grouped-Query Attention and ALiBi for positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        assert self.config.hidden_dim % self.config.n_head == 0\n",
        "        assert self.config.n_head % self.config.n_kv_head == 0\n",
        "        self.head_dim = self.config.hidden_dim // self.config.n_head\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_per_kv = self.config.n_head // self.config.n_kv_head\n",
        "\n",
        "        # Init projection layers\n",
        "        self.q_proj = todo()\n",
        "        self.kv_proj = todo()\n",
        "        self.out_proj = todo()\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(self.config.dropout)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", self._create_causal_mask(self.config.max_seq_len))\n",
        "        self.register_buffer(\"alibi\", self._build_alibi_bias(self.config.n_head))\n",
        "\n",
        "    def _build_alibi_bias(self, num_heads: int) -> Tensor:\n",
        "        \"\"\"Build ALiBi for specified number of heads:\n",
        "\n",
        "        Returns:\n",
        "            Tensor with ALiBi biases, shape: [1, num heads, 1, 1]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def _create_causal_mask(self, max_seq_len: int) -> Tensor:\n",
        "        \"\"\"Create causal mask with ones where tokens can attend to each other.\n",
        "\n",
        "        Returns:\n",
        "            Tensor with causal mask, shape: [1, 1, seq len, seq len]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Self-Attention to input data with respect to pad tokens.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Gated Liner Unit with Swish Activation\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # Init up- and down- projection layers\n",
        "        self.fc1 = todo()\n",
        "        self.fc2 = todo()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply SwiGLU to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Base Transformer Block\n",
        "        - Causal Self-Attention and SwiGLU as main elements\n",
        "        - Pre-normalization via RMSNorm\n",
        "        - Regularization with dropouts before residuals\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Transformer Block to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class TransformerForCausalLM(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = todo()\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = todo()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> Tensor:\n",
        "        \"\"\"Calculate logits for given input ids.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            logits, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Take a conditioning sequence of indices and complete the sequence max_new_tokens times,\n",
        "        feeding the predictions back into the model each time.\n",
        "\n",
        "        Args:\n",
        "            idx: tensor with conditional tokens, shape [seq len]\n",
        "            max_new_tokens: maximum number of new tokens\n",
        "            eos_token_id: index of EOS token to stop generation\n",
        "            temperature, do_sample, top_k: generation parameters\n",
        "        Return:\n",
        "            tensor with generated indexes\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len :]\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # 1. Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = todo()\n",
        "\n",
        "            # 2. Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                mask = todo()\n",
        "                logits[mask] = -float(\"inf\")\n",
        "\n",
        "            # 3. apply softmax to convert logits to probabilities\n",
        "            probs = todo()\n",
        "\n",
        "            # 4. Either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = todo()\n",
        "            else:\n",
        "                idx_next = todo()\n",
        "\n",
        "            # 5. Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdavMmSDujw"
      },
      "source": [
        "# Train Loop [2 балла]\n",
        "\n",
        "Настало время обучать модель.\n",
        "Небольшую можно пробовать обучать локально, но лучше всего воспользоваться GPU, например, на Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wtAiR_aDxed"
      },
      "outputs": [],
      "source": [
        "# Определим датасет и как заворачивать семплы в батч\n",
        "# Разные тексты имеют разную длину, поэтому будет падить до самого длина семпла\n",
        "# Так же заведем дополнительную маску, чтобы механизм внимания не учитывал падинги\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.texts[idx]\n",
        "        tokenized_sequence = self.tokenizer.encode(texts)\n",
        "        return tokenized_sequence\n",
        "\n",
        "\n",
        "def data_collator(\n",
        "    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    batch_size = len(tokenized_sequences)\n",
        "    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)\n",
        "    attention_mask = torch.zeros((batch_size, max_batch_seq_len))\n",
        "\n",
        "    for i, tok_seq in enumerate(tokenized_sequences):\n",
        "        cur_len = min(len(tok_seq), max_batch_seq_len)\n",
        "        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])\n",
        "        attention_mask[i, :cur_len] = 1\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):\n",
        "    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "_d = TextDataset([\"Привет!\", \"Как твои дела?\", \"Осталось совсем немного до конца\"], tokenizer)\n",
        "_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)\n",
        "\n",
        "for i, batch in enumerate(_dl):\n",
        "    print(f\"Batch #{i}\")\n",
        "    input_ids, attn_mask = batch\n",
        "    print(input_ids, attn_mask, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i719AOdQK993"
      },
      "outputs": [],
      "source": [
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Scheduler for Optimizer with linear warmup and linear decay to the end of training\n",
        "\n",
        "    Args:\n",
        "        optimizer: torch optimizer to control learning rate\n",
        "        num_warmup_steps: number of warmup steps\n",
        "        num_training_steps: total number of training steps\n",
        "    Return:\n",
        "        torch learning rate scheduler\n",
        "    \"\"\"\n",
        "    assert num_training_steps >= num_warmup_steps\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        todo()\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:\n",
        "    \"\"\"Calculate Cross-Entropy loss for Language Modeling task\n",
        "    Under the hood:\n",
        "    1. Create targtes based on input ids\n",
        "    2. Masked out tokens corresponded to paddings\n",
        "    3. Calculate cross entropy loss\n",
        "\n",
        "    Args:\n",
        "        input_ids: tensor with input ids, shape [bs, seq len]\n",
        "        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]\n",
        "        logits: predicted logits, shape [bs, seq len, vocab size]\n",
        "    Return:\n",
        "        cross entropy loss, single-item tensor\n",
        "    \"\"\"\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYdF52zXtoX"
      },
      "outputs": [],
      "source": [
        "# Определим тренера с наиболее важными гиперпараметрами для обучения\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        clip_grad_norm=1.0,\n",
        "        n_steps=10_000,\n",
        "        val_every_n_steps=1_000,\n",
        "        plot_every_n_steps=100,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.n_steps = n_steps\n",
        "        self.val_every_n_steps = val_every_n_steps\n",
        "        self.plot_every_n_steps = plot_every_n_steps\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, val_loader):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def run(self, model, train_loader, val_loader):\n",
        "        model = model.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "        plotlosses = PlotLosses(figsize=(15, 9), step_names=\"Step\")\n",
        "        logs = {\"lr\": 0, \"epoch\": 0}\n",
        "\n",
        "        data_iter = iter(train_loader)\n",
        "        for iter_num in range(self.n_steps):\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                logs[\"epoch\"] += 1\n",
        "                batch = next(data_iter)\n",
        "\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            loss = cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:\n",
        "                val_loss = self.validate(model, val_loader)\n",
        "                plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % self.plot_every_n_steps == 0:\n",
        "                logs[\"loss\"] = loss.item()\n",
        "                logs[\"lr\"] = scheduler.get_last_lr()[0]\n",
        "                plotlosses.update(logs, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "\n",
        "        val_loss = self.validate(model, val_loader)\n",
        "        plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "        plotlosses.send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1BpJflMTAi"
      },
      "outputs": [],
      "source": [
        "# Создаем тренировочный и тестовые даталоадеры\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = TextDataset(dataset[\"train\"][\"text\"], tokenizer)\n",
        "train_dataloader = create_dataloader(\n",
        "    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(dataset[\"test\"][\"text\"], tokenizer)\n",
        "test_dataloader = create_dataloader(\n",
        "    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jHSgMZECGl"
      },
      "outputs": [],
      "source": [
        "# Инициализируем модель\n",
        "\n",
        "config = model_configs[\"nano\"]\n",
        "model = TransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUsjHl4Nkoa"
      },
      "outputs": [],
      "source": [
        "# Инициализируем тренера\n",
        "\n",
        "trainer = Trainer(learning_rate=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUgUfpKK993"
      },
      "outputs": [],
      "source": [
        "# Обучение goes brrrr!\n",
        "\n",
        "trainer.run(model, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94zNDLEqdQow"
      },
      "outputs": [],
      "source": [
        "# Смотрим на качество генерации глазами\n",
        "# Для маленьких и слабых моделей \"затягиваем\" гайки генерации\n",
        "\n",
        "text = \"Заходит в бар\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]\n",
        "print(input_ids)\n",
        "model_output = model.generate(\n",
        "    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFi7M9ExHWv9"
      },
      "outputs": [],
      "source": [
        "# Загружаем модель на хаб\n",
        "\n",
        "model.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrMwV9zK993"
      },
      "source": [
        "Поиграйтесь с гиперпараметрами, попробуйте обучить `mini` и `small` версии.\n",
        "Постарайтесь добиться как можно более высокого качества как в терминах лосса, так и при визуальной оценке генерации.\n",
        "\n",
        "### Дополнительные баллы\n",
        "\n",
        "Вы также можно заработать дополнительные баллы:\n",
        "- Реализовать Rotary Positional Embedding **[2 балла]**\n",
        "- Реализовать Multi-Head Latent Attention **[1 балл]**\n",
        "- Оформить репозиторий на 🤗: карточка модели с описанием задания, репортом качества и примерами генерации **[1 балл]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8bIDy5dKXM"
      },
      "source": [
        "# Специальный раздел для проверяющего"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZplshN5HtRb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)\n",
        "check_model = TransformerForCausalLM.from_pretrained(REPO_NAME)\n",
        "check_model = check_model.to(device)\n",
        "check_model = check_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araF_3noK994"
      },
      "outputs": [],
      "source": [
        "text = \"Штирлиц пришел домой\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text), device=device)\n",
        "model_output = check_model.generate(\n",
        "    input_ids[None, :], max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CRSE4EFq3bX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание № 12\n",
        "\n",
        "Выполните все задания в этом ноутбуке.\n",
        "\n",
        "+ Мягкий дедлайн: `11.05.25 23:59`\n",
        "+ Жесткий дедлайн: `18.05.25 23:59` (половина баллов)\n",
        "\n",
        "\n",
        "После жесткого дедлайна задание не принимается."
      ],
      "metadata": {
        "id": "yU9It9c13cia"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_3_10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}