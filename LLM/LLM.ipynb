{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBA5IPyb4BFH"
      },
      "source": [
        "# LLM\n",
        "\n",
        "–ò–º—è, –§–∞–º–∏–ª–∏—è:\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnY6pwvpghE"
      },
      "source": [
        "–í —ç—Ç–æ–π –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç–µ –≤–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–∏—Ç—å Byte-level BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –Ω–µ–±–æ–ª—å—à—É—é LM.  \n",
        "\n",
        "–î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Ä—É—Å—Å–∫–∏–º–∏ –∞–Ω–µ–∫–¥–æ—Ç–∞–º–∏!\n",
        "\n",
        "–û–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏ –Ω—É–∂–Ω–æ –≤—ã–ª–æ–∂–∏—Ç—å –Ω–∞ [ü§ó HuggingFace](https://huggingface.co/). –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å —Ç–∞–º, –ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ [deep vk](https://huggingface.co/deepvk) –∏ —Å–æ–∑–¥–∞–π—Ç–µ —Å–µ–±–µ API —Ç–æ–∫–µ–Ω.\n",
        "\n",
        "–°–ª–µ–¥—É–π—Ç–µ —è—á–µ–π–∫–∞–º —Ç–µ—Ç—Ä–∞–¥–∫–∏ –∏ –∑–∞–ø–æ–ª–Ω—è–π—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —è—á–µ–π–∫–∏. –í –∫–æ–Ω—Ü–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏ –≤—ã –Ω–∞–π–¥–µ—Ç–µ –∑–∞–¥–∞—á–∏ —Å–æ –∑–≤–µ–∑–¥–æ—á–∫–æ–π, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0byNYx5dzB4b"
      },
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "\n",
        "%pip install --quiet datasets livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILR1tu3z9oI"
      },
      "outputs": [],
      "source": [
        "# –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
        "\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from functools import lru_cache, partial\n",
        "from pathlib import Path\n",
        "\n",
        "import regex as re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, PyTorchModelHubMixin, interpreter_login, snapshot_download\n",
        "from huggingface_hub.utils import SoftTemporaryDirectory\n",
        "from livelossplot import PlotLosses\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7OLSivnPW0"
      },
      "outputs": [],
      "source": [
        "# –≠—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –±—É–¥—É—Ç –ø–æ–º–µ—á–µ–Ω—ã –≤—Å–µ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–∑–∞–ø–æ–ª–Ω–∏—Ç—å\n",
        "# –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ —Ü–µ–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —Ç–∞–∫ –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö\n",
        "# –í—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏–µ–π –∏ –Ω–∞–π—Ç–∏ –º–µ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ :)\n",
        "\n",
        "\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkJJp2JK99x"
      },
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVWKkwaryDTq"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –±—É–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/llm-course-hw1\"  # –ò–ª–∏ –∫–∞–∫ –≤–∞–º —Ö–æ—á–µ—Ç—Å—è\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")\n",
        "\n",
        "# –ò –¥—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –≤–µ—â–∏\n",
        "SEED = 0xC0FFEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxN5JUbZ3ToV"
      },
      "source": [
        "# –î–∞—Ç–∞—Å–µ—Ç\n",
        "\n",
        "–ü–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ: [ü§ó IgorVolochay/russian_jokes](https://huggingface.co/datasets/IgorVolochay/russian_jokes)\n",
        "\n",
        "–ò –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–∏—Ö üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT78JNcqpRXW"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"IgorVolochay/russian_jokes\")\n",
        "print(\"\\n===\\n\".join(dataset[\"train\"][\"text\"][:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7G7o6OdK99z"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ö–æ–ª–¥–∞—É—Ç—ã\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzvdEVO3-kM"
      },
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä [3 –±–∞–ª–ª–∞]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç Byte-level BPE.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ:\n",
        "1. –†–µ–∞–ª–∏–∑—É–µ–º –µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–∞–±–æ—Ä —Å–ª–∏—è–Ω–∏–π –ø–æ —ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—é\n",
        "2. –û–±—É—á–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "3. –†–µ–∞–ª–∏–∑—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15U6H1iLU3kI"
      },
      "outputs": [],
      "source": [
        "# –í—Å—è–∫–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏\n",
        "\n",
        "WHITESPACE_SPLITTER = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "\n",
        "def bytes_to_unicode() -> dict[int, str]:\n",
        "    \"\"\"The original dictionary consists of 256 bytes and their corresponding Unicode characters.\n",
        "    For example, chr(33) is '!'. However, not all bytes have a visually appealing representation,\n",
        "    so such characters are skipped and replaced with the first available ones, i.e. shifted by 256.\n",
        "    \"\"\"\n",
        "    initial_bytes = (\n",
        "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n",
        "    )\n",
        "    initial_chars = [chr(it) for it in initial_bytes]\n",
        "    n = 0\n",
        "    for byte in range(2**8):\n",
        "        if byte not in initial_bytes:\n",
        "            initial_bytes.append(byte)\n",
        "            initial_chars.append(chr(2**8 + n))\n",
        "            n += 1\n",
        "    return dict(sorted(zip(initial_bytes, initial_chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk919hENEFwL"
      },
      "outputs": [],
      "source": [
        "def merge(merge_pair: tuple[str, str], pair_frequences: Counter[tuple[str, str]], words_by_tokens: Counter[tuple[str]]):\n",
        "    \"\"\"Merges a given pair of tokens and update corresponding stats\n",
        "\n",
        "    Args:\n",
        "        merge_pair: The pair of tokens to be merged.\n",
        "        pair_frequences: A counter tracking the frequency of token pairs in the dataset.\n",
        "        words_by_tokens: A counter mapping tokenized words to their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        Updated pair frequences and word tokenization w.r.t. to new token.\n",
        "    \"\"\"\n",
        "    todo()\n",
        "\n",
        "\n",
        "def train(data: list[str], vocab_size: int = 1024, special_tokens: list[str] = None):\n",
        "    \"\"\"Train BPE tokenizer on passed data\n",
        "\n",
        "    Args:\n",
        "        data: List of train documents\n",
        "        vocab_size: Size of target vocabulary\n",
        "        special_tokens: List of special tokens to add into vocabulary\n",
        "    Returns:\n",
        "        vocabulary: mapping from string token to id\n",
        "        merges: list of merges, each one is tuple of string tokens\n",
        "    \"\"\"\n",
        "    if vocab_size < 256:\n",
        "        raise ValueError(\"Vocab size can't be less than 256\")\n",
        "    if special_tokens is None:\n",
        "        special_tokens = []\n",
        "\n",
        "    # 1. Initialize vocabulary (using inverse one during training)\n",
        "    id2token = bytes_to_unicode()\n",
        "    merges = []\n",
        "\n",
        "    # 2. Load data\n",
        "    words_by_tokens = Counter()\n",
        "    for sample in tqdm(data, desc=\"Loading data\"):\n",
        "        # 2.1 Split into words\n",
        "        words = WHITESPACE_SPLITTER.findall(sample.strip())\n",
        "        for word in words:\n",
        "            # 2.2 Tokenize with base vocabulary\n",
        "            todo()\n",
        "\n",
        "    # 3. Calculate statistic of token's pairs\n",
        "    pair_frequences = Counter()\n",
        "    todo()\n",
        "\n",
        "    # 4. Build vocabulary\n",
        "    pbar = trange(vocab_size, desc=\"Building vocabulary\", initial=len(id2token) + len(special_tokens))\n",
        "    while len(id2token) < vocab_size - len(special_tokens):\n",
        "        if len(pair_frequences) == 0:\n",
        "            print(\"Not enough data to fulfil vocabulary\")\n",
        "            break\n",
        "\n",
        "        # 4.1 Find the most frequent pair and create new token\n",
        "        top_pair = todo()\n",
        "        new_token = todo()\n",
        "        del pair_frequences[top_pair]\n",
        "\n",
        "        # 4.2 Add to vocabulary\n",
        "        if new_token in id2token.values():\n",
        "            continue\n",
        "        id2token[len(id2token)] = new_token\n",
        "        merges.append(top_pair)\n",
        "\n",
        "        # 4.3 Update stats and merge the top pair in all tokens\n",
        "        pair_frequences, words_by_tokens = merge(top_pair, pair_frequences, words_by_tokens)\n",
        "\n",
        "        pbar.update()\n",
        "    pbar.close()\n",
        "\n",
        "    # 5. Add special tokens\n",
        "    for special_token in special_tokens:\n",
        "        id2token[len(id2token)] = special_token\n",
        "\n",
        "    return {v: k for k, v in id2token.items()}, merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLwur-KgK990"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö\n",
        "# –î–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏ —Ö–≤–∞—Ç–∏—Ç –∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –Ω–æ –º–æ–∂–µ—Ç–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∏—Ç—å!\n",
        "\n",
        "\n",
        "vocab, merges = train(dataset[\"train\"][\"text\"], vocab_size=1024, special_tokens=[\"[EOS]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcsapJUAK990"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "\n",
        "random_tokens = [512, 614, 768, 888, 1022]\n",
        "unicode_to_bytes = {v: k for k, v in bytes_to_unicode().items()}\n",
        "for token_id in random_tokens:\n",
        "    token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "    raw_bytes = bytes([unicode_to_bytes[it] for it in token])\n",
        "    print(f\"Token #{token_id}: '{raw_bytes.decode('utf-8', errors='replace')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUz7cma1Czs"
      },
      "outputs": [],
      "source": [
        "class ByteLevelBPETokenizer:\n",
        "\n",
        "    def __init__(self, vocab: dict[str, int], merges: list[tuple[str, str]], eos_token: str = \"[EOS]\"):\n",
        "        \"\"\"Byte-Level BPE Tokenizer\n",
        "\n",
        "        Args:\n",
        "            vocab: mapping from string token to id\n",
        "            merges: list of merges in prioritized order\n",
        "            eos_token: string representation of EOS token\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if eos_token not in vocab:\n",
        "            raise ValueError(\"There is no EOS token in vocab\")\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.token2id = vocab\n",
        "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
        "        self.eos_token = eos_token\n",
        "        self.eos_token_id = self.token2id[eos_token]\n",
        "\n",
        "        # The closer the pair is to the beginning, the higher the rank\n",
        "        self.merges = merges\n",
        "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "\n",
        "    @lru_cache\n",
        "    def bpe(self, word: tuple[str]) -> tuple[str]:\n",
        "        \"\"\"Process word into tokenized representation.\n",
        "        Word is a tuple of base tokens, i.e. bytes.\n",
        "\n",
        "        Under the hood:\n",
        "        1. Tracks the set of token pairs, bi-grams\n",
        "        2. While possible, replaces the highest-ranking pair with its union\n",
        "\n",
        "        Args:\n",
        "            word: list of base string tokens\n",
        "        Return:\n",
        "            list of BPE tokens\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def encode(self, text: str, add_eos_token: bool = True) -> list[int]:\n",
        "        \"\"\"Convert string to list of token ids.\n",
        "\n",
        "        Args:\n",
        "            text: input string, may contain multiple words\n",
        "            add_eos_token: whether to add eos token id at the end\n",
        "        Return:\n",
        "            list of ints, ids of tokenized text\n",
        "        \"\"\"\n",
        "        words = WHITESPACE_SPLITTER.findall(text)\n",
        "        todo()\n",
        "\n",
        "    def decode(self, idx: list[int]) -> str:\n",
        "        \"\"\"Convert list of tokens' ids to text, opposite to encode method\n",
        "\n",
        "        Args:\n",
        "            idx: list of tokens' ids\n",
        "        Return:\n",
        "            string, decoded text\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def push_to_hub(self, repo_id, *, private=None, token=None):\n",
        "        api = HfApi()\n",
        "        repo_id = api.create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True).repo_id\n",
        "\n",
        "        # Push the files to the repo in a single commit\n",
        "        with SoftTemporaryDirectory() as tmp:\n",
        "            save_directory = Path(tmp) / repo_id\n",
        "            save_directory.mkdir(parents=True)\n",
        "            with open(save_directory / \"vocabulary.json\", \"w\") as f_out:\n",
        "                print(json.dumps(self.token2id, indent=2), file=f_out)\n",
        "            with open(save_directory / \"merges.json\", \"w\") as f_out:\n",
        "                print(json.dumps({\"merges\": self.merges}), file=f_out)\n",
        "\n",
        "            return api.upload_folder(repo_id=repo_id, folder_path=save_directory, token=token)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *, token=None, **model_kwargs):\n",
        "        if not os.path.isdir(pretrained_model_name_or_path):\n",
        "            storage_folder = snapshot_download(repo_id=pretrained_model_name_or_path, token=token)\n",
        "        else:\n",
        "            storage_folder = pretrained_model_name_or_path\n",
        "        storage_folder = Path(storage_folder)\n",
        "        with open(storage_folder / \"vocabulary.json\", \"r\") as f_in:\n",
        "            vocab = json.load(f_in)\n",
        "        with open(storage_folder / \"merges.json\", \"r\") as f_in:\n",
        "            merges = [tuple(it) for it in json.load(f_in)[\"merges\"]]\n",
        "        return cls(vocab, merges, **model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTQzO3wK991"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(vocab, merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9m_65vBK991"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ö–∞–±\n",
        "\n",
        "tokenizer.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9VohtKrK991"
      },
      "outputs": [],
      "source": [
        "# –°–∫–∞—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å —Ö–∞–±–∞\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvf_Q8knPW4"
      },
      "outputs": [],
      "source": [
        "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "\n",
        "text = \"–°—Ç—É–¥–µ–Ω—Ç—ã –í–®–≠ –∑–Ω–∞—é—Ç, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏–∫–∞ ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –Ω–∞ –ª–µ–∫—Ü–∏–∏ —Ç—ã —Ç–µ—Ä—è–µ—à—å –≤—Ä–µ–º—è, –∞ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ ‚Äî –Ω–∞–¥–µ–∂–¥—É.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "reverse_text = [tokenizer.decode([it]) for it in ids]\n",
        "print(\"|\".join(reverse_text))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgpwFYiK991"
      },
      "outputs": [],
      "source": [
        "# –ü–æ—Å—á–∏—Ç–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏–º—Å—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É –º–æ–¥–µ–ª–∏\n",
        "\n",
        "lens = []\n",
        "for text in tqdm(dataset[\"test\"][\"text\"]):\n",
        "    ids = tokenizer.encode(text)\n",
        "    lens.append(len(ids))\n",
        "\n",
        "print(f\"Average token len per sample: {sum(lens) / len(lens):.2f}\")\n",
        "print(f\"Minimum and maximum lens are: {min(lens)} and {max(lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwN3mfmznPW5"
      },
      "source": [
        "–î–æ–ª–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å—Å—è –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ 70 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.\n",
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 128 —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥–µ—Ç –≤–ø–æ–ª–Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1FljGXC7I-"
      },
      "source": [
        "# –ú–æ–¥–µ–ª—å [5 –±–∞–ª–ª–æ–≤]\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª–∏–∑—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º\n",
        "1. –í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ALiBi\n",
        "2. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GQA\n",
        "3. –í Feed-Forward –±–ª–æ–∫–µ SwiGLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipZrCvkmK992"
      },
      "outputs": [],
      "source": [
        "# –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∑–∞–≤–µ–¥–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_kv_head: int\n",
        "    hidden_dim: int\n",
        "    intermediate_dim: int\n",
        "    dropout: float = 0.1\n",
        "    vocab_size: int = 1024\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"nano\": TransformerConfig(n_layer=3, n_head=4, n_kv_head=2, hidden_dim=96, intermediate_dim=256),\n",
        "    \"mini\": TransformerConfig(n_layer=6, n_head=6, n_kv_head=3, hidden_dim=384, intermediate_dim=1024),\n",
        "    \"small\": TransformerConfig(n_layer=12, n_head=12, n_kv_head=6, hidden_dim=768, intermediate_dim=2048),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYXMb5PEDFkt"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        \"\"\"Root Mean Square Layer Normalization\n",
        "\n",
        "        Args:\n",
        "            dim: Feature dimension\n",
        "            eps: Small constant for numerical stability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        todo()\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Causal Self-Attention with support of\n",
        "        Grouped-Query Attention and ALiBi for positional encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        assert self.config.hidden_dim % self.config.n_head == 0\n",
        "        assert self.config.n_head % self.config.n_kv_head == 0\n",
        "        self.head_dim = self.config.hidden_dim // self.config.n_head\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        self.q_per_kv = self.config.n_head // self.config.n_kv_head\n",
        "\n",
        "        # Init projection layers\n",
        "        self.q_proj = todo()\n",
        "        self.kv_proj = todo()\n",
        "        self.out_proj = todo()\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(self.config.dropout)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", self._create_causal_mask(self.config.max_seq_len))\n",
        "        self.register_buffer(\"alibi\", self._build_alibi_bias(self.config.n_head))\n",
        "\n",
        "    def _build_alibi_bias(self, num_heads: int) -> Tensor:\n",
        "        \"\"\"Build ALiBi for specified number of heads:\n",
        "\n",
        "        Returns:\n",
        "            Tensor with ALiBi biases, shape: [1, num heads, 1, 1]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def _create_causal_mask(self, max_seq_len: int) -> Tensor:\n",
        "        \"\"\"Create causal mask with ones where tokens can attend to each other.\n",
        "\n",
        "        Returns:\n",
        "            Tensor with causal mask, shape: [1, 1, seq len, seq len]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Self-Attention to input data with respect to pad tokens.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Gated Liner Unit with Swish Activation\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # Init up- and down- projection layers\n",
        "        self.fc1 = todo()\n",
        "        self.fc2 = todo()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply SwiGLU to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Base Transformer Block\n",
        "        - Causal Self-Attention and SwiGLU as main elements\n",
        "        - Pre-normalization via RMSNorm\n",
        "        - Regularization with dropouts before residuals\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_1 = nn.Dropout(config.dropout)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = RMSNorm(config.hidden_dim)\n",
        "        self.res_dropout_2 = nn.Dropout(config.dropout)\n",
        "        self.mlp = SwiGLU(config)\n",
        "\n",
        "    def forward(self, x: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"Apply Transformer Block to input data.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            result tensor, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "\n",
        "class TransformerForCausalLM(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        \"\"\"Transformer model for Language Modeling\"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.max_seq_len = config.max_seq_len\n",
        "        self.n_layer = config.n_layer\n",
        "        self.n_head = config.n_head\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.token_emb = todo()\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_final = RMSNorm(config.hidden_dim)\n",
        "        self.lm_head = todo()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params / 1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            torch.nn.init.ones_(module.scale)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> Tensor:\n",
        "        \"\"\"Calculate logits for given input ids.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor, shape [bs, seq len, hidden dim]\n",
        "            attention_mask: mask with zeros for pad tokens, shape [bs, seq len, hidden dim]\n",
        "        Returns:\n",
        "            logits, shape [bs, seq len, hidden dim]\n",
        "        \"\"\"\n",
        "        todo()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self, idx: Tensor, max_new_tokens, eos_token_id, temperature=1.0, do_sample=False, top_k=None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Take a conditioning sequence of indices and complete the sequence max_new_tokens times,\n",
        "        feeding the predictions back into the model each time.\n",
        "\n",
        "        Args:\n",
        "            idx: tensor with conditional tokens, shape [seq len]\n",
        "            max_new_tokens: maximum number of new tokens\n",
        "            eos_token_id: index of EOS token to stop generation\n",
        "            temperature, do_sample, top_k: generation parameters\n",
        "        Return:\n",
        "            tensor with generated indexes\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.shape[1] <= self.max_seq_len else idx[:, -self.max_seq_len :]\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # 1. Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = todo()\n",
        "\n",
        "            # 2. Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                mask = todo()\n",
        "                logits[mask] = -float(\"inf\")\n",
        "\n",
        "            # 3. apply softmax to convert logits to probabilities\n",
        "            probs = todo()\n",
        "\n",
        "            # 4. Either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = todo()\n",
        "            else:\n",
        "                idx_next = todo()\n",
        "\n",
        "            # 5. Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if idx_next == eos_token_id:\n",
        "                break\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdavMmSDujw"
      },
      "source": [
        "# Train Loop [2 –±–∞–ª–ª–∞]\n",
        "\n",
        "–ù–∞—Å—Ç–∞–ª–æ –≤—Ä–µ–º—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å.\n",
        "–ù–µ–±–æ–ª—å—à—É—é –º–æ–∂–Ω–æ –ø—Ä–æ–±–æ–≤–∞—Ç—å –æ–±—É—á–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è GPU, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wtAiR_aDxed"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–∞–∫ –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å —Å–µ–º–ø–ª—ã –≤ –±–∞—Ç—á\n",
        "# –†–∞–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –¥–ª–∏–Ω—É, –ø–æ—ç—Ç–æ–º—É –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–∞ —Å–µ–º–ø–ª–∞\n",
        "# –¢–∞–∫ –∂–µ –∑–∞–≤–µ–¥–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –º–∞—Å–∫—É, —á—Ç–æ–±—ã –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª –ø–∞–¥–∏–Ω–≥–∏\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.texts[idx]\n",
        "        tokenized_sequence = self.tokenizer.encode(texts)\n",
        "        return tokenized_sequence\n",
        "\n",
        "\n",
        "def data_collator(\n",
        "    tokenized_sequences: list[list[int]], pad_token_id: int, max_seq_len: int = None\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    batch_size = len(tokenized_sequences)\n",
        "    max_batch_seq_len = min(max_seq_len, max((len(it) for it in tokenized_sequences)))\n",
        "\n",
        "    input_ids = torch.full((batch_size, max_batch_seq_len), pad_token_id)\n",
        "    attention_mask = torch.zeros((batch_size, max_batch_seq_len))\n",
        "\n",
        "    for i, tok_seq in enumerate(tokenized_sequences):\n",
        "        cur_len = min(len(tok_seq), max_batch_seq_len)\n",
        "        input_ids[i, :cur_len] = torch.tensor(tok_seq[:cur_len])\n",
        "        attention_mask[i, :cur_len] = 1\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, pad_token_id, max_seq_len, batch_size, is_train):\n",
        "    collate_fn = partial(data_collator, pad_token_id=pad_token_id, max_seq_len=max_seq_len)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=is_train, drop_last=is_train, collate_fn=collate_fn, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "_d = TextDataset([\"–ü—Ä–∏–≤–µ—Ç!\", \"–ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?\", \"–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–æ –∫–æ–Ω—Ü–∞\"], tokenizer)\n",
        "_dl = create_dataloader(_d, tokenizer.eos_token_id, max_seq_len=16, batch_size=2, is_train=False)\n",
        "\n",
        "for i, batch in enumerate(_dl):\n",
        "    print(f\"Batch #{i}\")\n",
        "    input_ids, attn_mask = batch\n",
        "    print(input_ids, attn_mask, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i719AOdQK993"
      },
      "outputs": [],
      "source": [
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Scheduler for Optimizer with linear warmup and linear decay to the end of training\n",
        "\n",
        "    Args:\n",
        "        optimizer: torch optimizer to control learning rate\n",
        "        num_warmup_steps: number of warmup steps\n",
        "        num_training_steps: total number of training steps\n",
        "    Return:\n",
        "        torch learning rate scheduler\n",
        "    \"\"\"\n",
        "    assert num_training_steps >= num_warmup_steps\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        todo()\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(input_ids: Tensor, attention_mask: Tensor, logits: Tensor) -> Tensor:\n",
        "    \"\"\"Calculate Cross-Entropy loss for Language Modeling task\n",
        "    Under the hood:\n",
        "    1. Create targtes based on input ids\n",
        "    2. Masked out tokens corresponded to paddings\n",
        "    3. Calculate cross entropy loss\n",
        "\n",
        "    Args:\n",
        "        input_ids: tensor with input ids, shape [bs, seq len]\n",
        "        attention_mask: mask with zeros for pad tokens, shape [bs, seq len]\n",
        "        logits: predicted logits, shape [bs, seq len, vocab size]\n",
        "    Return:\n",
        "        cross entropy loss, single-item tensor\n",
        "    \"\"\"\n",
        "    todo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYdF52zXtoX"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª–∏–º —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        clip_grad_norm=1.0,\n",
        "        n_steps=10_000,\n",
        "        val_every_n_steps=1_000,\n",
        "        plot_every_n_steps=100,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.clip_grad_norm = clip_grad_norm\n",
        "        self.n_steps = n_steps\n",
        "        self.val_every_n_steps = val_every_n_steps\n",
        "        self.plot_every_n_steps = plot_every_n_steps\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, model, val_loader):\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            val_loss += cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "        return val_loss / len(val_loader)\n",
        "\n",
        "    def run(self, model, train_loader, val_loader):\n",
        "        model = model.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=0.1 * self.n_steps, num_training_steps=self.n_steps\n",
        "        )\n",
        "        model.train()\n",
        "\n",
        "        plotlosses = PlotLosses(figsize=(15, 9), step_names=\"Step\")\n",
        "        logs = {\"lr\": 0, \"epoch\": 0}\n",
        "\n",
        "        data_iter = iter(train_loader)\n",
        "        for iter_num in range(self.n_steps):\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                logs[\"epoch\"] += 1\n",
        "                batch = next(data_iter)\n",
        "\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(self.device, non_blocking=True)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)  # [bs; seq len; vocab size]\n",
        "            loss = cross_entropy_loss(input_ids, attention_mask, logits)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if iter_num > 0 and iter_num % self.val_every_n_steps == 0:\n",
        "                val_loss = self.validate(model, val_loader)\n",
        "                plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "                model.train()\n",
        "\n",
        "            if iter_num % self.plot_every_n_steps == 0:\n",
        "                logs[\"loss\"] = loss.item()\n",
        "                logs[\"lr\"] = scheduler.get_last_lr()[0]\n",
        "                plotlosses.update(logs, current_step=iter_num)\n",
        "                plotlosses.send()\n",
        "\n",
        "        val_loss = self.validate(model, val_loader)\n",
        "        plotlosses.update({\"val_loss\": val_loss.item()}, current_step=iter_num)\n",
        "        plotlosses.send()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1BpJflMTAi"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = TextDataset(dataset[\"train\"][\"text\"], tokenizer)\n",
        "train_dataloader = create_dataloader(\n",
        "    train_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=True\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(dataset[\"test\"][\"text\"], tokenizer)\n",
        "test_dataloader = create_dataloader(\n",
        "    test_dataset, tokenizer.eos_token_id, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE, is_train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jHSgMZECGl"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
        "\n",
        "config = model_configs[\"nano\"]\n",
        "model = TransformerForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUsjHl4Nkoa"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞\n",
        "\n",
        "trainer = Trainer(learning_rate=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUgUfpKK993"
      },
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ goes brrrr!\n",
        "\n",
        "trainer.run(model, train_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94zNDLEqdQow"
      },
      "outputs": [],
      "source": [
        "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–ª–∞–∑–∞–º–∏\n",
        "# –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∏ —Å–ª–∞–±—ã—Ö –º–æ–¥–µ–ª–µ–π \"–∑–∞—Ç—è–≥–∏–≤–∞–µ–º\" –≥–∞–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "\n",
        "text = \"–ó–∞—Ö–æ–¥–∏—Ç –≤ –±–∞—Ä\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text)[:-1], device=trainer.device)[None, :]\n",
        "print(input_ids)\n",
        "model_output = model.generate(\n",
        "    input_ids, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFi7M9ExHWv9"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ö–∞–±\n",
        "\n",
        "model.push_to_hub(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrMwV9zK993"
      },
      "source": [
        "–ü–æ–∏–≥—Ä–∞–π—Ç–µ—Å—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å `mini` –∏ `small` –≤–µ—Ä—Å–∏–∏.\n",
        "–ü–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –¥–æ–±–∏—Ç—å—Å—è –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–∫ –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –ª–æ—Å—Å–∞, —Ç–∞–∫ –∏ –ø—Ä–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã\n",
        "\n",
        "–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã:\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Rotary Positional Embedding **[2 –±–∞–ª–ª–∞]**\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Multi-Head Latent Attention **[1 –±–∞–ª–ª]**\n",
        "- –û—Ñ–æ—Ä–º–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ ü§ó: –∫–∞—Ä—Ç–æ—á–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∑–∞–¥–∞–Ω–∏—è, —Ä–µ–ø–æ—Ä—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ **[1 –±–∞–ª–ª]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8bIDy5dKXM"
      },
      "source": [
        "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZplshN5HtRb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer.from_pretrained(REPO_NAME)\n",
        "check_model = TransformerForCausalLM.from_pretrained(REPO_NAME)\n",
        "check_model = check_model.to(device)\n",
        "check_model = check_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araF_3noK994"
      },
      "outputs": [],
      "source": [
        "text = \"–®—Ç–∏—Ä–ª–∏—Ü –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π\"\n",
        "input_ids = torch.tensor(tokenizer.encode(text), device=device)\n",
        "model_output = check_model.generate(\n",
        "    input_ids[None, :], max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10\n",
        ")\n",
        "tokenizer.decode(model_output[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CRSE4EFq3bX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ 12\n",
        "\n",
        "–í—ã–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –∑–∞–¥–∞–Ω–∏—è –≤ —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ.\n",
        "\n",
        "+ –ú—è–≥–∫–∏–π –¥–µ–¥–ª–∞–π–Ω: `11.05.25 23:59`\n",
        "+ –ñ–µ—Å—Ç–∫–∏–π –¥–µ–¥–ª–∞–π–Ω: `18.05.25 23:59` (–ø–æ–ª–æ–≤–∏–Ω–∞ –±–∞–ª–ª–æ–≤)\n",
        "\n",
        "\n",
        "–ü–æ—Å–ª–µ –∂–µ—Å—Ç–∫–æ–≥–æ –¥–µ–¥–ª–∞–π–Ω–∞ –∑–∞–¥–∞–Ω–∏–µ –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è."
      ],
      "metadata": {
        "id": "yU9It9c13cia"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_3_10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}